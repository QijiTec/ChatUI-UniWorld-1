import gradio as gr
import sys
sys.path.append("..")
from transformers import AutoProcessor, SiglipImageProcessor, SiglipVisionModel, T5EncoderModel, BitsAndBytesConfig
from univa.models.qwen2p5vl.modeling_univa_qwen2p5vl import UnivaQwen2p5VLForConditionalGeneration
from univa.utils.flux_pipeline import FluxPipeline
from univa.utils.get_ocr import get_ocr_result
from univa.utils.denoiser_prompt_embedding_flux import encode_prompt
from qwen_vl_utils import process_vision_info
from univa.utils.anyres_util import dynamic_resize, concat_images_adaptive
import torch
from torch import nn
import os
import uuid
import base64
from typing import Dict
from PIL import Image, ImageDraw, ImageFont

import argparse

def parse_args():
    parser = argparse.ArgumentParser(description="Model and component paths")

    parser.add_argument("--model_path", type=str, default="LanguageBind/UniWorld-V1", help="UniWorld-V1æ¨¡å‹è·¯å¾„")
    parser.add_argument("--flux_path", type=str, default="black-forest-labs/FLUX.1-dev", help="FLUX.1-devæ¨¡å‹è·¯å¾„")
    parser.add_argument("--siglip_path", type=str, default="google/siglip2-so400m-patch16-512", help="siglip2æ¨¡å‹è·¯å¾„")
    parser.add_argument("--lora_path", type=str, default="loras", help="Flux LoRAæ¨¡å‹è·¯å¾„")
    parser.add_argument("--server_name", type=str, default="127.0.0.1", help="IPåœ°å€")
    parser.add_argument("--server_port", type=int, default=6812, help="ç«¯å£å·")
    parser.add_argument("--share", action="store_true", help="æ˜¯å¦å…¬å¼€åˆ†äº«")
    parser.add_argument("--nf4", action="store_true", help="æ˜¯å¦NF4é‡åŒ–")


    return parser.parse_args()


def add_plain_text_watermark(
    img: Image.Image,
    text: str,
    margin: int = 50, 
    font_size: int = 30, 
):
    if img.mode != "RGB":
        img = img.convert("RGB")

    draw = ImageDraw.Draw(img)
    font = ImageFont.truetype("DejaVuSans.ttf", font_size)
    bbox = draw.textbbox((0, 0), text)
    text_width = bbox[2] - bbox[0]
    text_height = bbox[3] - bbox[1]

    x = img.width - text_width - int(3.3 * margin)
    y = img.height - text_height - margin

    draw.text((x, y), text, font=font, fill=(255, 255, 255))
    return img


css = """
.table-wrap table tr td:nth-child(3) > div {
    max-height: 150px;       /* æœ€å¤š 100px é«˜åº¦ï¼ŒæŒ‰éœ€ä¿®æ”¹ */
    overflow-y: auto;        /* è¶…å‡ºéƒ¨åˆ†æ˜¾ç¤ºç«–å‘æ»šåŠ¨æ¡ */
    white-space: pre-wrap;   /* è‡ªåŠ¨æ¢è¡Œ */
    word-break: break-all;   /* é•¿å•è¯å†…éƒ¨åˆ†è¡Œ */
}

.table-wrap table tr td:nth-child(2) > div {
    max-width: 150px;
    white-space: pre-wrap;
    word-break: break-all;
    overflow-x: auto;
}
.table-wrap table tr th:nth-child(2) {
    max-width: 150px;
    white-space: normal;
    word-break: keep-all;
    overflow-x: auto;
}

.table-wrap table tr td:nth-last-child(-n+8) > div {
    max-width: 130px;
    white-space: pre-wrap;
    word-break: break-all;
    overflow-x: auto;
}
.table-wrap table tr th:nth-last-child(-n+8) {
    max-width: 130px;
    white-space: normal;
    word-break: keep-all;
    overflow-x: auto;
}
"""


def img2b64(image_path):
    with open(image_path, "rb") as f:
        b64 = base64.b64encode(f.read()).decode()
    data_uri = f"data:image/jpeg;base64,{b64}"
    return data_uri


def initialize_models(args):
    os.makedirs("outputs", exist_ok=True)
    # Paths
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_quant_type="nf4",
    )
    
    # Load main model and task head
    model = UnivaQwen2p5VLForConditionalGeneration.from_pretrained(
        args.model_path,
        torch_dtype=torch.bfloat16,
        attn_implementation="flash_attention_2",
        quantization_config=quantization_config if args.nf4 else None,
    ).to(device)
    task_head = nn.Sequential(
        nn.Linear(3584, 10240),
        nn.SiLU(),
        nn.Dropout(0.3),
        nn.Linear(10240, 2)
    ).to(device)
    task_head.load_state_dict(torch.load(os.path.join(args.model_path, 'task_head_final.pt')))
    task_head.eval()

    processor = AutoProcessor.from_pretrained(
        args.model_path,
        min_pixels=448*448,
        max_pixels=448*448,
    )
    if args.nf4:
        text_encoder_2 = T5EncoderModel.from_pretrained(
            args.flux_path,
            subfolder="text_encoder_2",
            quantization_config=quantization_config,
            torch_dtype=torch.bfloat16,
        )
        pipe = FluxPipeline.from_pretrained(
            args.flux_path,
            transformer=model.denoise_tower.denoiser,
            text_encoder_2=text_encoder_2,
            torch_dtype=torch.bfloat16,
        ).to(device)
    else:
        pipe = FluxPipeline.from_pretrained(
            args.flux_path,
            transformer=model.denoise_tower.denoiser,
            torch_dtype=torch.bfloat16,
        ).to(device)
        
    # é¢„åŠ è½½ LoRA æ¨¡å‹ä½†é»˜è®¤ä¸å¯ç”¨
    pipe.load_lora_weights(os.path.join(args.lora_path, "Turbo_8Steps.safetensors"), adapter_name="nitro")
    pipe.load_lora_weights(os.path.join(args.lora_path, "NSFW_master.safetensors"), adapter_name="nsfw")
    pipe.load_lora_weights(os.path.join(args.lora_path, "METAGIRL-FLUX.safetensors"), adapter_name="girl")
    pipe.load_lora_weights(os.path.join(args.lora_path, "ICEdit-normal.safetensors"), adapter_name="icedit")
    pipe.load_lora_weights(os.path.join(args.lora_path, "Detailer.safetensors"), adapter_name="detailer")
    pipe.load_lora_weights(os.path.join(args.lora_path, "RedCraft-RED1.safetensors"), adapter_name="redcraft")
    
    # é»˜è®¤ç¦ç”¨æ‰€æœ‰ LoRA
    pipe.disable_lora()
    tokenizers = [pipe.tokenizer, pipe.tokenizer_2]
    text_encoders = [pipe.text_encoder, pipe.text_encoder_2]
    pipe.enable_model_cpu_offload()
    pipe.enable_vae_slicing()

    # Optional SigLIP
    siglip_processor, siglip_model = None, None
    siglip_processor = SiglipImageProcessor.from_pretrained(args.siglip_path)
    siglip_model = SiglipVisionModel.from_pretrained(
        args.siglip_path,
        torch_dtype=torch.bfloat16,
    ).to(device)

    return {
        'model': model,
        'task_head': task_head,
        'processor': processor,
        'pipe': pipe,
        'tokenizers': tokenizers,
        'text_encoders': text_encoders,
        'siglip_processor': siglip_processor,
        'siglip_model': siglip_model,
        'device': device,
    }


args = parse_args()
state = initialize_models(args)


def process_large_image(raw_img):
    if raw_img is None:
        return raw_img
    img = Image.open(raw_img).convert("RGB")

    max_side = max(img.width, img.height)
    if max_side > 1024:
        scale = 1024 / max_side
        new_w = int(img.width * scale)
        new_h = int(img.height * scale)
        print(f'resize img {img.size} to {(new_w, new_h)}')
        img = img.resize((new_w, new_h), resample=Image.LANCZOS)
        save_path = f"outputs/{uuid.uuid4().hex}.png"
        img.save(save_path)
        return save_path
    else:
        return raw_img


def chat_step(image1, image2, text, height, width, steps, guidance,
              ocr_enhancer, joint_with_t5, enhance_generation, enhance_understanding,
              enable_nitro, enable_nsfw, enable_girl, enable_icedit, 
              enable_detailer, enable_redcraft, seed, num_imgs, history_state, progress=gr.Progress()):
    
    try:
        convo = history_state['conversation']
        image_paths = history_state['history_image_paths']
        cur_ocr_i = history_state['cur_ocr_i']
        cur_genimg_i = history_state['cur_genimg_i']

        # image1 = process_large_image(image1)
        # image2 = process_large_image(image2)
        # Build content
        content = []
        if text:
            ocr_text = ''
            if ocr_enhancer and content:
                ocr_texts = []
                for img in (image1, image2):
                    if img:
                        ocr_texts.append(get_ocr_result(img, cur_ocr_i))
                        cur_ocr_i += 1
                ocr_text = '\n'.join(ocr_texts)
            content.append({'type':'text','text': text + ocr_text})
        for img in (image1, image2):
            if img:
                content.append({'type':'image','image':img,'min_pixels':448*448,'max_pixels':448*448})
                image_paths.append(img)

        convo.append({'role':'user','content':content})

        # Prepare inputs
        chat_text = state['processor'].apply_chat_template(convo,
                        tokenize=False, add_generation_prompt=True)
        chat_text = '<|im_end|>\n'.join(chat_text.split('<|im_end|>\n')[1:])
        image_inputs, video_inputs = process_vision_info(convo)
        inputs = state['processor'](
            text=[chat_text], images=image_inputs, videos=video_inputs,
            padding=True, return_tensors='pt'
        ).to(state['device'])

        # Model forward & task head
        with torch.no_grad():
            outputs = state['model'](**inputs, return_dict=True, output_hidden_states=True)
        hidden = outputs.hidden_states[-1]
        mask = inputs.input_ids == 77091
        vecs = hidden[mask][-1:]
        task_res = state['task_head'](vecs.float())[0]
        print(task_res)
        # Branch decision
        if enhance_generation:
            do_image = True
        elif enhance_understanding:
            do_image = False
        else:
            do_image = (task_res[0] < task_res[1])

        seed = int(seed)
        if seed == -1:
            seed = torch.Generator(device="cpu").seed()
        torch.manual_seed(seed)
        # Generate
        if do_image:
            # image generation pipeline
            siglip_hs = None
            if state['siglip_processor'] and image_paths:
                vals = [state['siglip_processor'].preprocess(
                            images=Image.open(p).convert('RGB'), do_resize=True,
                            return_tensors='pt', do_convert_rgb=True
                        ).pixel_values.to(state['device'])
                        for p in image_paths]
                siglip_hs = state['siglip_model'](torch.concat(vals)).last_hidden_state

            with torch.no_grad():
                lvlm = state['model'](
                    inputs.input_ids, pixel_values=getattr(inputs,'pixel_values',None),
                    attention_mask=inputs.attention_mask,
                    image_grid_thw=getattr(inputs,'image_grid_thw',None),
                    siglip_hidden_states=siglip_hs,
                    output_type='denoise_embeds'
                )
                prm_embeds, pooled = encode_prompt(
                    state['text_encoders'], state['tokenizers'],
                    text if joint_with_t5 else '', 256, state['device'], 1
                )
            emb = torch.concat([lvlm, prm_embeds], dim=1) if joint_with_t5 else lvlm


            def diffusion_to_gradio_callback(_pipeline, step_idx: int, timestep: int, tensor_dict: Dict):
                # 1ï¼‰æ›´æ–° Gradio è¿›åº¦æ¡
                frac = (step_idx + 1) / float(steps)
                progress(frac)

                return tensor_dict

            # æ ¹æ® Checkbox çŠ¶æ€æ§åˆ¶ LoRA æ¨¡å‹çš„å¯ç”¨/ç¦ç”¨
            pipe = state['pipe']
            pipe.disable_lora()  # å…ˆç¦ç”¨æ‰€æœ‰ LoRA
            
            # æ ¹æ® Checkbox çŠ¶æ€å¯ç”¨å¯¹åº”çš„ LoRA
            if enable_nitro:
                pipe.set_adapters("nitro")
                pipe.enable_lora()
                print("Enabled nitro LoRA")
            
            if enable_nsfw:
                pipe.set_adapters("nsfw")
                pipe.enable_lora()
                print("Enabled NSFW LoRA")
                
            if enable_girl:
                pipe.set_adapters("girl")
                pipe.enable_lora()
                print("Enabled girl LoRA")
            
            if enable_icedit:
                pipe.set_adapters("icedit")
                pipe.enable_lora()
                print("Enabled icedit LoRA")
                
            if enable_detailer:
                pipe.set_adapters("detailer")
                pipe.enable_lora()
                print("Enabled detailer LoRA")
                
            if enable_redcraft:
                pipe.set_adapters("redcraft")
                pipe.enable_lora()
                print("Enabled redcraft LoRA")
                
            with torch.no_grad():
                img = pipe(
                    prompt_embeds=emb, pooled_prompt_embeds=pooled,
                    height=height, width=width,
                    num_inference_steps=steps,
                    guidance_scale=guidance,
                    generator=torch.Generator(device='cuda').manual_seed(seed), 
                    num_images_per_prompt=num_imgs, 
                    callback_on_step_end=diffusion_to_gradio_callback,
                    # callback_on_step_end_tensor_inputs=["latents", "prompt_embeds"],
                ).images
            # img = [add_plain_text_watermark(im, 'Open-Sora Plan 2.0 Generated') for im in img]
            img = concat_images_adaptive(img)
            save_path = f"outputs/{uuid.uuid4().hex}.png"
            img.save(save_path)
            convo.append({'role':'assistant','content':[{'type':'image','image':save_path}]})
            cur_genimg_i += 1
            progress(1.0)
            bot_msg = (None, save_path)
        else:
            # text generation
            gen_ids = state['model'].generate(**inputs, max_new_tokens=128)
            out = state['processor'].batch_decode(
                [g[len(inputs.input_ids[0]):] for g in gen_ids], skip_special_tokens=True
            )[0]
            convo.append({'role':'assistant','content':[{'type':'text','text':out}]})
            bot_msg = (None, out)

        
        chat_pairs = []
        # print(convo)
        # print()
        # print()
        for msg in convo:
            # print(msg)
            if msg['role']=='user':
                parts = []
                for c in msg['content']:
                    if c['type']=='text': parts.append(c['text'])
                    if c['type']=='image': parts.append(f"![user image]({img2b64(c['image'])})")
                chat_pairs.append(("\n".join(parts), None))
            else:
                parts = []
                for c in msg['content']:
                    if c['type']=='text': parts.append(c['text'])
                    if c['type']=='image': parts.append(f"![assistant image]({img2b64(c['image'])})")
                if msg['content'][-1]['type']=='text':
                    chat_pairs[-1] = (chat_pairs[-1][0], parts[-1])
                else:
                    chat_pairs[-1] = (chat_pairs[-1][0], parts[-1])
        # print()
        # print(chat_pairs)

        # Update state
        history_state.update({
            'conversation': convo,
            'history_image_paths': image_paths,
            'cur_ocr_i': cur_ocr_i,
            'cur_genimg_i': cur_genimg_i
        })
        return chat_pairs, history_state, seed
    except Exception as e:
        # æ•æ‰æ‰€æœ‰å¼‚å¸¸ï¼Œè¿”å›é”™è¯¯æç¤ºï¼Œå»ºè®®ç”¨æˆ·æ¸…ç†å†å²åé‡è¯•
        error_msg = f"å‘ç”Ÿé”™è¯¯ï¼š{e}. è¯·ç‚¹å‡» \"Clear History\" æ¸…ç†å¯¹è¯å†å²åå†è¯•ä¸€æ¬¡ã€‚"
        chat_pairs = [(None, error_msg)]
        # ä¸ä¿®æ”¹ history_stateï¼Œè®©ç”¨æˆ·è‡ªè¡Œæ¸…ç†
        return chat_pairs, history_state, seed

def copy_seed_for_user(real_seed):
    # è¿™ä¸ªå‡½æ•°ä¼šæŠŠéšè—çš„ seed_holder å€¼ï¼Œä¼ ç»™çœŸæ­£è¦æ˜¾ç¤ºçš„ seed Textbox
    return real_seed

def clear_inputs():
    # img1 å’Œ img2 ç”¨ None æ¥æ¸…ç©ºï¼›text_in ç”¨ç©ºå­—ç¬¦ä¸²æ¸…ç©ºï¼›seed åŒç†æ¸…ç©º
    return None, None, "", ""

def clear_history():
    # é»˜è®¤ prompt å’Œ seed
    default_prompt = "è¾“å…¥åæŒ‰å›è½¦å‘é€..."
    default_seed   = "-1"

    # 1. chatbot è¦ç”¨ gr.update(value=[]) æ¸…ç©º
    # 2. state ç›´æ¥ç»™å›åˆå§‹ dict
    # 3. prompt å’Œ seed åŒæ ·ç”¨ gr.update()
    return (
        gr.update(value=[]),                             # æ¸…ç©ºèŠå¤©æ¡†
        {'conversation':[],                              # é‡ç½® state
         'history_image_paths':[],
         'cur_ocr_i':0,
         'cur_genimg_i':0},
        gr.update(value=None),                 # é‡ç½® image1
        gr.update(value=None),                 # é‡ç½® image2
        gr.update(value=default_prompt),                 # é‡ç½® prompt æ–‡æœ¬æ¡†
        gr.update(value=default_seed),                   # é‡ç½® seed æ–‡æœ¬æ¡†
    )


if __name__ == '__main__':
    # Gradio UI
    with gr.Blocks(
        theme=gr.themes.Soft(), 
        css=css
        ) as demo:

        gr.Markdown(
            """
            <div style="text-align:center;">
            #CHATUI.uniworld-v1
            </div>
            """,
        )

        with gr.Row():
            with gr.Column():
                chatbot = gr.Chatbot(
                    max_height=100000, min_height=550, 
                    height=None, 
                    resizable=True, 
                    show_copy_button=True
                    )
                text_in = gr.Textbox(label="å¯¹è¯å¼•å¯¼ Instruction", value="è¾“å…¥åæŒ‰å›è½¦å‘é€...")
                with gr.Accordion("Adapters Options", open=True, visible=True):
                    with gr.Row():
                        enable_nitro = gr.Checkbox(value=False, label="Enable Nitro Boost - 8-10steps")
                        enable_nsfw = gr.Checkbox(value=False, label="Enable NSFW - for T2Igeneration")
                        enable_girl = gr.Checkbox(value=False, label="Enable Asian Girl")
                    with gr.Row():
                        enable_icedit = gr.Checkbox(value=False, label="Enable ICEdit")
                        enable_detailer = gr.Checkbox(value=False, label="Enable Detailer")
                        enable_redcraft = gr.Checkbox(value=False, label="Enable RedCraft style")
            with gr.Column():
                with gr.Row():
                    img1 = gr.Image(type='filepath', label="Image 1", height=256, width=256)
                    img2 = gr.Image(type='filepath', label="Image 2 (Optional reference)", height=256, width=256, visible=True)
                seed = gr.Textbox(label="Seed (-1 for random)", value="-1")
                seed_holder = gr.Textbox(visible=False)
                with gr.Row():
                    num_imgs = gr.Slider(1, 4, 1, step=1, label="Num Images")
                
                with gr.Row():
                    height = gr.Slider(256, 2048, 768, step=64, label="Height")
                    width = gr.Slider(256, 2048, 768, step=64, label="Width")
                with gr.Row():
                    steps = gr.Slider(8, 50, 28, step=1, label="Inference steps")
                    guidance = gr.Slider(1.0, 10.0, 4.0, step=0.1, label="Guidance scale")
                with gr.Accordion("Advanced Options", open=True, visible=True):
                    with gr.Row():
                        enhance_gen_box = gr.Checkbox(value=False, label="Enhance Generation")
                        enhance_und_box = gr.Checkbox(value=False, label="Enhance Understanding")
                    with gr.Row():
                        ocr_box = gr.Checkbox(value=False, label="Enhance Text Rendering")
                        t5_box = gr.Checkbox(value=False, label="Enhance Current Turn")

        anchor_pixels = 1024*1024
        # Dynamic resize callback
        def update_size(i1, i2):
            shapes = []
            for p in (i1, i2):
                if p:
                    im = Image.open(p)
                    w, h = im.size
                    shapes.append((w, h))
            if not shapes:
                return gr.update(), gr.update()
            if len(shapes) == 1:
                w, h = shapes[0]
            else:
                w = sum(s[0] for s in shapes) / len(shapes)
                h = sum(s[1] for s in shapes) / len(shapes)
            new_h, new_w = dynamic_resize(int(h), int(w), 'any_11ratio', anchor_pixels=anchor_pixels)
            return gr.update(value=new_h), gr.update(value=new_w)
        img1.change(fn=update_size, inputs=[img1, img2], outputs=[height, width])
        img2.change(fn=update_size, inputs=[img1, img2], outputs=[height, width])

        # Mutual exclusivity
        enhance_und_box.change(
            lambda u: gr.update(value=False) if u else gr.update(),
            inputs=[enhance_und_box], outputs=[enhance_gen_box]
        )
        enhance_gen_box.change(
            lambda g: gr.update(value=False) if g else gr.update(),
            inputs=[enhance_gen_box], outputs=[enhance_und_box]
        )

        state_ = gr.State({'conversation':[], 'history_image_paths':[], 'cur_ocr_i':0, 'cur_genimg_i':0})
        with gr.Row():
            submit = gr.Button("Send", variant="primary")
            clear = gr.Button("Clear History", variant="primary")



        progress_bar = gr.Progress()
        click_event = submit.click(
            fn=chat_step,
            inputs=[img1, img2, text_in, height, width, steps, guidance,
                    ocr_box, t5_box, enhance_gen_box, enhance_und_box,
                    enable_nitro, enable_nsfw, enable_girl, enable_icedit, 
                    enable_detailer, enable_redcraft,seed, num_imgs, state_,
                    ],
            outputs=[chatbot, state_, seed_holder], 
            scroll_to_output=True
        )
        
        click_event.then(
            fn=copy_seed_for_user,
            inputs=[seed_holder],    # è¾“å…¥æ˜¯éšè—çš„ seed_holder
            outputs=[seed]           # è¾“å‡ºåˆ°çœŸæ­£è¦æ˜¾ç¤ºçš„ seed Textbox
        )

        clear.click(
            fn=clear_history,
            inputs=[],
            outputs=[chatbot, state_, img1, img2, text_in, seed]
        )

        submit_event = text_in.submit(
            fn=chat_step,
            inputs=[img1, img2, text_in, height, width, steps, guidance,
                    ocr_box, t5_box, enhance_gen_box, enhance_und_box,
                    enable_nitro, enable_nsfw, enable_girl, enable_icedit, 
                    enable_detailer, enable_redcraft,seed, num_imgs, state_,
                    ],
            outputs=[chatbot, state_, seed_holder], 
            scroll_to_output=True
        )
        
        submit_event.then(
            fn=copy_seed_for_user,
            inputs=[seed_holder],    # è¾“å…¥æ˜¯éšè—çš„ seed_holder
            outputs=[seed]           # è¾“å‡ºåˆ°çœŸæ­£è¦æ˜¾ç¤ºçš„ seed Textbox
        )

        with gr.Row():
            with gr.Column(1, min_width=0):
                gr.Markdown(
                    """
                    **ğŸ–¼ï¸ è§†è§‰æ„ŸçŸ¥ä¸ç‰¹å¾æå– / Visual Perception & Feature Extraction**  
                    - Canny è¾¹ç¼˜æ£€æµ‹ / Canny Edge Detection  
                    - å°å°ºåº¦çº¿æ®µæ£€æµ‹ / Mini-Line Segment Detection  
                    - æ³•çº¿å›¾ç”Ÿæˆ / Normal Map Generation  
                    - è‰å›¾ç”Ÿæˆ / Sketch Generation  
                    - å…¨å±€åµŒå¥—è¾¹ç¼˜æ£€æµ‹ / Holistically-Nested Edge Detection  
                    - æ·±åº¦ä¼°è®¡ / Depth Estimation  
                    - äººä½“å§¿æ€ä¼°è®¡ / Human Pose Estimation  
                    - ç›®æ ‡æ£€æµ‹ï¼ˆæ¡†ï¼‰/ Object Detection (Boxes)  
                    - è¯­ä¹‰åˆ†å‰²ï¼ˆæ©ç ï¼‰/ Semantic Segmentation (Masks)
                    """
                )
            with gr.Column(1, min_width=0):
                gr.Markdown(
                    """
                    **âœ‚ï¸ å›¾åƒç¼–è¾‘ä¸å¤„ç† / Image Editing & Manipulation**  
                    - æ·»åŠ å…ƒç´  / Add Elements  
                    - è°ƒæ•´å±æ€§ / Adjust Attributes  
                    - æ›´æ¢èƒŒæ™¯ / Change Background  
                    - ç§»é™¤å¯¹è±¡ / Remove Objects  
                    - æ›¿æ¢åŒºåŸŸ / Replace Regions  
                    - æ‰§è¡ŒåŠ¨ä½œ / Perform Actions  
                    - é‡æ–°é£æ ¼åŒ– / Restyle  
                    - æ„å»ºåœºæ™¯ / Compose Scenes
                    """
                )
            with gr.Column(1, min_width=0):
                gr.Markdown(
                    """
                    **ğŸ”„ è·¨æ¨¡æ€åˆæˆä¸è½¬æ¢ / Cross-Modal Synthesis & Transformation**  
                    - æ–‡æœ¬ç”Ÿæˆå›¾åƒ / Textâ†’Image Synthesis  
                    - å›¾åƒåˆ°å›¾åƒç¿»è¯‘ / Image-to-Image Translation  
                    - å¤šå›¾èåˆç”Ÿæˆ / Multi-Image Combination  
                    - æå– IP ç‰¹å¾ / Extract IP Features  
                    - IP ç‰¹å¾ç»„åˆ / IP Feature Composition
                    """ 
                )
            with gr.Column(1, min_width=0):
                gr.Markdown(
                    """
                    **ğŸ¤– è§†è§‰ä¸æ–‡æœ¬é—®ç­” / Visual & Textual QA**  
                    - å›¾åƒ-æ–‡æœ¬é—®ç­” / Image-Text QA  
                    - æ–‡æœ¬-æ–‡æœ¬é—®ç­” / Text-Text QA
                    """
                )

        # ========== æ·»åŠ  Validation Examples ==========
        example_height, example_width = 768, 768
        gr.Examples(
            examples_per_page=100, 
            examples=[
                # text-to-image
                [None, None,
                "Generate an adorable golden retriever puppy playing in a sunny park, "
                "with fluffy fur, big round eyes, and a happy expression. "
                "The background should have green grass, some flowers, and a blue sky with white clouds.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],


                # NIKE color swap
                ["assets/nike_src.jpg", None,
                "Switch the product's color from black, black to white, white, making sure the transition is crisp and clear.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],

                # style transfer (Ghibli)
                ["assets/gradio/origin.png", None,
                "Translate this photo into a Studio Ghibli-style illustration, holding true to the original composition and movement.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],

                ["assets/gradio/origin.png", None,
                "Remove the bicycle located in the lower center region of the image.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],

                # blur
                ["assets/gradio/blur.jpg", None,
                "Remove blur, make it clear.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],

                # 
                ["assets/gradio/00004614_tgt.jpg", None,
                "Add the ingrid fair isle cashmere turtleneck sweater to the person.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],
                # 
                ["assets/gradio/00006581_tgt.jpg", None,
                "Place the belvoir broderie anglaise linen tank on the person in a way that complements their appearance and style.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],
                # 
                ["assets/gradio/00008153_tgt.jpg", None,
                "Integrate may cashmere tank on body.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],
                # 
                ["assets/gradio/00002315_src.jpg", None,
                "Strip away all context and distractions, leaving the pointelle-trimmed cashmere t-shirt floating on a neutral background.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],
                # 
                ["assets/gradio/00002985_src.jpg", None,
                "Generate an image containing only the henry shearling jacket, free from any other visual elements.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],

                ["assets/gradio/origin.png", None,
                "Add a cat in the center of image.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],

                # image+image-to-image (compose)
                ["assets/00182555_target.jpg",
                "assets/00182555_InstantStyle_ref_1.jpg",
                "Adapt Image1's content to fit the aesthetic of Image2.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],

                # replace object
                ["assets/replace_src.png", None,
                "replace motorcycle located in the lower center region of the image with a black bicycle",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],

                # segmentation
                ["assets/seg_src.jpg", None,
                "Segment the giraffe from the background.\n",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],

                # detection
                ["assets/det_src.jpg", None,
                "Please depict the vase accurately",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],

                # image-to-canny
                ["assets/canny_image.jpg", None,
                "Generate a Canny edge map for this image.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],

                # image-to-mlsd
                ["assets/mlsd_image.jpg", None,
                "Render an MLSD detection overlay for this input image.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],

                # image-to-normal
                ["assets/normal_image.jpg", None,
                "Convert the input texture into a tangent-space normal map.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],

                # image-to-sketch
                ["assets/sketch_image.jpg", None,
                "Transform this image into a hand-drawn charcoal sketch.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],

                # image-to-hed
                ["assets/hed_image.jpg", None,
                "Produce a holistically-nested boundary probability map of this image.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],

                # image-to-depth
                ["assets/depth_image.jpg", None,
                "Estimate depth with a focus on background structure.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],
                
                # image-to-image (reconstruction)
                ["assets/rec.jpg", None,
                "Simply reconstruct the original image with no enhancements.",
                example_height, example_width, 25, 4.0, False, False, False, False, "-1", 1],

            ],
            inputs=[img1, img2, text_in, height, width, steps, guidance,
                    ocr_box, t5_box, enhance_gen_box, enhance_und_box, seed, num_imgs],
        )
    # ==============================================

        gr.Markdown(
            """
            <div style="text-align:center;">
            ###è§£é”å°–ç«¯è§†è§‰æ„ŸçŸ¥ã€ç‰¹å¾æå–ã€ç¼–è¾‘ã€åˆæˆä¸ç†è§£

            **ä½¿ç”¨æŒ‡å—ï¼š**

            - å»ºè®®åŒæ—¶å¯¹å››å¼ å›¾ç‰‡è¿›è¡Œæ¨ç†ï¼Œä»¥æä¾›å¤šæ ·åŒ–çš„é€‰æ‹©ã€‚

            - ä¸Šä¼ çš„å›¾åƒä¼šè‡ªåŠ¨è°ƒæ•´å°ºå¯¸ï¼›ä¸å»ºè®®æ‰‹åŠ¨æŒ‡å®šä¸åŸå›¾å·®å¼‚è¿‡å¤§çš„åˆ†è¾¨ç‡ã€‚
            </div>
            """,
            elem_classes="header-text",
        )



if __name__ == "__main__": 
    demo.launch(
        allowed_paths=["/"],
        server_name=args.server_name, 
        server_port=args.server_port,
        share=args.share, 
        inbrowser=True,
    )


'''

MODEL_PATH="/mnt/data/lb/Remake/FlowWorld/checkpoints/flux_qwen2p5vl_7b_vlm_mlp_siglip_stage2_ts_1024_bs42x8x1_fa_any_11ratio_ema999_ocr_adamw_t5_0p4_lr1e-5_mask_refstyle_extract_resume_run3/checkpoint-12000/model_ema"
FLUX_PATH="/mnt/data/checkpoints/black-forest-labs/FLUX.1-dev"
SIGLIP_PATH="/mnt/data/checkpoints/google/siglip2-so400m-patch16-512"
CUDA_VISIBLE_DEVICES=2 python -m univa.serve.gradio_web_server \
    --model_path ${MODEL_PATH} \
    --flux_path ${FLUX_PATH} \
    --siglip_path ${SIGLIP_PATH}

'''
